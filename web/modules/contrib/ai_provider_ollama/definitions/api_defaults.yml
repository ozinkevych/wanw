chat:
  input:
    description: 'Input provided to the model.'
    type: 'array'
    default:
      - { role: "system", content: "You are a helpful assistant." }
      - { role: "user", content: "Introduce yourself!" }
    required: true
  configuration:
    max_tokens:
      label: 'Max Tokens'
      description: 'The maximum number of tokens that can be generated in the chat completion.'
      type: 'integer'
      default: 1024
      required: false
    temperature:
      label: 'Temperature'
      description: 'Sampling temperature 0-1. Higher values mean more random output.'
      type: 'float'
      default: 1
      required: false
      constraints:
        min: 0
        max: 2
        step: 0.1
    frequency_penalty:
      label: 'Frequency Penalty'
      description: 'Number between -2.0 and 2.0. Positive values penalize new tokens based on existing frequency in the text so far.'
      type: 'integer'
      default: 0
      required: false
      constraints:
        min: -2
        max: 2
        step: 0.1
    presence_penalty:
      label: 'Presence Penalty'
      description: 'Number between -2.0 and 2.0. Positive values penalize new tokens on whether they appear in the text so far.'
      type: 'integer'
      default: 0
      required: false
      constraints:
        min: -2
        max: 2
        step: 0.1
    top_p:
      label: 'Top P'
      description: 'An alternative to sampling with temperature, called nucleus sampling.'
      type: 'float'
      default: 1
      required: false
      constraints:
        min: 0
        max: 1
        step: 0.1
embeddings:
  input:
    description: 'Textual representation of the speech.'
    type: 'string'
    default: 'Once upon a time in London.'
    required: true
  configuration: []
